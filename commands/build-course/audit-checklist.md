# 模拟通关审计清单

课程生成后，逐关检查以下维度。

## 审计维度

### 1. 技术可行性

每关设计的操作，在 skill 的 allowed-tools 范围内是否能实现？

常见问题：

- 需要 Task 工具但 allowed-tools 没有（无法 spawn agent）→ 改为模拟演示
- 需要网络请求但没有 WebFetch → 改为本地操作
- 需要特定 CLI 工具但用户可能没安装 → 加入第1关的环境检测

### 2. 信息密度

每关展示给用户的信息量是否均衡？

| 密度等级 | 描述                    | 适用关卡     |
| -------- | ----------------------- | ------------ |
| 轻       | 几行提示 + 自动检测     | 第1关        |
| 中       | 讲解 + 选择题/决策      | 第2关、第4关 |
| 重       | 完整交互流程 + 产出展示 | 第3关        |
| 中偏重   | 手写 + 生成 + 审查      | 第5关        |

红线：

- 单关输出超过 2000 字符的连续文本 → 需要拆分或精简
- 连续两关都是"重"密度 → 中间加入轻量关卡过渡
- 展示全文 vs 展示摘要 → 优先摘要，全文用链接引导

### 3. 参与度递进

检查5关的用户参与方式是否符合递进规律：

```
第1关：跟着做（被动执行）
  ↓
第2关：看 + 选（知识测验）
  ↓
第3关：亲手操作（实战产出）
  ↓
第4关：观察 + 决策（判断力）
  ↓
第5关：亲手创造（从无到有）
```

红线：

- 第5关没有手写环节 → 必须加入（至少写一句话）
- 第3关和第4关都是纯观看 → 第3关必须有实际操作
- 连续3关都是选择题 → 缺乏实战，需要重新设计

### 4. 验证设计

每关的通过条件是否：

- **可执行**：能通过命令/文件检查自动验证
- **有意义**：验证的是学习目标，不是形式
- **不过严**：不要因为小问题阻塞用户进度

验证方式参考：

| 类型     | 适用          | 示例                                   |
| -------- | ------------- | -------------------------------------- |
| 自动检测 | 环境/文件存在 | `ls xxx \| wc -l`                      |
| 选择题   | 概念理解      | 3题对2题                               |
| 文件产出 | 实战操作      | 指定目录有新文件 + 文件内容 > N 字符   |
| 交互完成 | 决策参与      | 用户完成了 N 个 AskUserQuestion        |
| 内容验证 | 创造产出      | 文件存在 + frontmatter 有效 + 行数 > N |

### 5. 前后关联

检查关卡之间是否有素材复用链：

- 第3关的产出是否被第4关复用？
- 第2关讲的概念是否在第5关被回顾？
- 缺少关联 → 课程会感觉像5个独立练习而非一段旅程

## 审计报告格式

```
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
🔍 课程审计报告：{课程名}
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

| 关卡 | 技术可行 | 信息密度 | 参与度 | 验证设计 | 关联性 |
|------|---------|---------|--------|---------|--------|
| 第1关 | ✅/⚠️/❌ | 轻/中/重 | 类型 | ✅/⚠️ | — |
| ...  |         |         |        |         |        |

发现的问题：
  1. {问题描述} → 建议：{修复方案}
  2. ...

整体评估：
  ✅ 参与度递进：{是否符合}
  ✅ 总耗时预估：{XX 分钟}
  ✅ 信息密度均衡：{是否}
  ⚠️ {如有整体性问题}
━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
```
